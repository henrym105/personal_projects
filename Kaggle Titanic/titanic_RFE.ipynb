{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "(418, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the train data set\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr         517\n",
      "Miss       185\n",
      "Mrs        126\n",
      "Master      40\n",
      "Officer     18\n",
      "Royalty      5\n",
      "Name: Title, dtype: int64\n",
      "(714, 18)\n",
      "x_train shape = (714, 17)\n",
      "y_train shape = (714,)\n",
      "Index(['Age', 'Fare', 'Sex_female', 'Sex_male'], dtype='object')\n",
      "[ 2  1  4  5  1  1  1  7 12  9 14 10  6  3  8 11 13]\n"
     ]
    }
   ],
   "source": [
    "# Load the train data set\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Can simplify the Titles with this dictionary:\n",
    "Title_Dictionary = {\"Capt\"  : \"Officer\", \n",
    "                    \"Col\"   : \"Officer\",\n",
    "                    \"Major\" : \"Officer\",\n",
    "                    \"Dr\"    : \"Officer\", \n",
    "                    \"Rev\"   : \"Officer\", \n",
    "                    \"Don\"   : \"Royalty\", \n",
    "                    \"Sir\"   : \"Royalty\", \n",
    "                    \"Jonkheer\": \"Royalty\",\n",
    "                    \"the Countess\":\"Royalty\", \n",
    "                    \"Mr\"    : \"Mr\", \n",
    "                    \"Mme\"   : \"Mrs\", \n",
    "                    \"Mrs\"   : \"Mrs\", \n",
    "                    \"Ms\"    : \"Miss\", \n",
    "                    \"Miss\"  : \"Miss\", \n",
    "                    \"Mlle\"  : \"Miss\", \n",
    "                    \"Master\" : \"Master\", \n",
    "                    \"Lady\"  : \"Royalty\" \n",
    "                    }\n",
    "    \n",
    "# adding summarized \"Title\" column to data\n",
    "train[\"Title\"] = train['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "train[\"Title\"] = train[\"Title\"].map(Title_Dictionary)\n",
    "print(train[\"Title\"].value_counts())\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "for i in ('Sex','Embarked', 'Title'):\n",
    "    # Create a one-hot encoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Encode the \"Sex\" column\n",
    "    encoder.fit(train[[i]])\n",
    "    sex_encoded = encoder.transform(train[[i]])\n",
    "    sex_encoded_df = pd.DataFrame(sex_encoded.toarray(), columns=encoder.get_feature_names_out([i]))\n",
    "\n",
    "    # Join the encoded column to the original dataframe\n",
    "    train = pd.concat([train, sex_encoded_df], axis=1)\n",
    "\n",
    "    # Drop the original \"Sex\" column\n",
    "    train = train.drop(columns=[i])\n",
    "    # print(train.head(2))\n",
    "\n",
    "# train.head(3)\n",
    "\n",
    "# NOTE Could also do it this way\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# # One limitations of this: it changes the column categorical values into numerical values, but then the model may interpret these numerical values as \n",
    "# # being ordinal.\n",
    "# # If the data is not ordered (as in  the Sex here, there's no reason that male should be 1 and female 0, or vice versa) you should\n",
    "# # use OneHot encoder instead\n",
    "\n",
    "# # Create a label encoder\n",
    "# encoder = LabelEncoder()\n",
    "\n",
    "# # Encode the non-numeric columns\n",
    "# train['Sex'] = encoder.fit_transform(train['Sex']\n",
    "\n",
    "# train['Embarked'] = encoder.fit_transform(train['Sex'])\n",
    "# train.head()\n",
    "# train[\"Embarked\"].value_counts()\n",
    "\n",
    "\n",
    "# Drop the unnecessary columns, then drop the NA values\n",
    "\n",
    "train = train.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'])\n",
    "# print(train.head())\n",
    "train = train.dropna()\n",
    "print(train.shape)\n",
    "\n",
    "# Split the data into X and y\n",
    "x_train = train.drop(columns=['Survived'])\n",
    "y_train = train['Survived']\n",
    "\n",
    "# x_train = x_train.dropna()\n",
    "# x_train = x_train.dropna()\n",
    "\n",
    "print(f\"x_train shape = {x_train.shape}\")\n",
    "print(f\"y_train shape = {y_train.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# CREATING THE MODEL: Sklearn Random Forest Classifier \n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Use RFE (Recursive Factor Elimination) to select the top 4 features\n",
    "my_RF_model = RFE(estimator=rf, n_features_to_select=4)\n",
    "my_RF_model.fit(x_train, y_train)\n",
    "\n",
    "# Print the selected features\n",
    "print(x_train.columns[my_RF_model.support_])\n",
    "\n",
    "# print(x_train.columns)\n",
    "print(my_RF_model.ranking_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Test\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test shape = (418, 11)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: DOING IT ALL AGAIN FOR TEST DATA:\n",
    "\n",
    "# Load the train data set\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "print(f\"Original Test shape = {test.shape}\")\n",
    "\n",
    "\n",
    "# adding summarized \"Title\" column to data\n",
    "test[\"Title\"] = test['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "test[\"Title\"] = test[\"Title\"].map(Title_Dictionary)\n",
    "# print(test[\"Title\"].value_counts())\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "for i in ('Sex','Embarked', 'Title'):\n",
    "    # Create a one-hot encoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Encode the current column\n",
    "    encoder.fit(test[[i]])\n",
    "    sex_encoded = encoder.transform(test[[i]])\n",
    "    sex_encoded_df = pd.DataFrame(sex_encoded.toarray(), columns=encoder.get_feature_names_out([i]))\n",
    "\n",
    "    # Join the encoded column to the original dataframe\n",
    "    test = pd.concat([test, sex_encoded_df], axis=1)\n",
    "\n",
    "    # Drop the original column\n",
    "    test = test.drop(columns=[i])\n",
    "    # print(train.head(2))\n",
    "\n",
    "test = test.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'])\n",
    "\n",
    "\n",
    "\n",
    "# print(test.columns)\n",
    "# column_list = x_train.columns[my_RF_model.support_]\n",
    "# # print(x_train.columns[my_RF_model.support_])\n",
    "# print(column_list)\n",
    "\n",
    "# x_test = test[column_list]\n",
    "# print(x_test.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test = test[my_RF_model.support_]\n",
    "# test.columns\n",
    "# test = test.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'])\n",
    "\n",
    "# print(test.head())\n",
    "# print(f\"Final Test shape = {test.shape}\")\n",
    "# print(f\"y_test shape = {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a random forest classifier\n",
    "# rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Use RFE to select the top 4 features\n",
    "\n",
    "# Create a random forest classifier\n",
    "# rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# # Use RFE (Recursive Factor Elimination) to select the top 4 features\n",
    "# my_RF_model = RFE(estimator=rf, n_features_to_select=5)\n",
    "# my_RF_model.fit(x_train, y_train)\n",
    "\n",
    "# # Print the selected features\n",
    "# print(x_train.columns[my_RF_model.support_])\n",
    "\n",
    "# # print(x_train.columns)\n",
    "# print(my_RF_model.ranking_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Henry/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- Title_nan\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Embarked_nan\n",
      "- Title_Royalty\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Henry/Desktop/personal_projects/Kaggle Titanic/titanic_RFE.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Henry/Desktop/personal_projects/Kaggle%20Titanic/titanic_RFE.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# my_RF_model.fit(te, train_y)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Henry/Desktop/personal_projects/Kaggle%20Titanic/titanic_RFE.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_predicted \u001b[39m=\u001b[39m my_RF_model\u001b[39m.\u001b[39;49mpredict(test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Henry/Desktop/personal_projects/Kaggle%20Titanic/titanic_RFE.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(y_predicted)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[39mraise\u001b[39;00m attr_err\n\u001b[1;32m    112\u001b[0m     \u001b[39m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(obj, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:336\u001b[0m, in \u001b[0;36mRFE.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39m\"\"\"Reduce X to the selected features and then predict using the underlying estimator.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \n\u001b[1;32m    325\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m    The predicted target values.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    335\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 336\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator_\u001b[39m.\u001b[39mpredict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(X))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_base.py:83\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\"\"\"Reduce X to the selected features.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m    The input samples with only the selected features.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m# note: we use _safe_tags instead of _get_tags because this is a\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# public Mixin.\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m     84\u001b[0m     X,\n\u001b[1;32m     85\u001b[0m     dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     86\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     87\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m _safe_tags(\u001b[39mself\u001b[39;49m, key\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow_nan\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     88\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 566\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    567\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    568\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    795\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    797\u001b[0m         )\n\u001b[1;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 800\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    802\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    803\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m         allow_nan\n\u001b[1;32m    109\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[1;32m    110\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    111\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[1;32m    112\u001b[0m     ):\n\u001b[1;32m    113\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# my_RF_model.fit(te, train_y)\n",
    "\n",
    "y_predicted = my_RF_model.predict(test)\n",
    "print(y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(y_predicted, columns = ['Survived'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"PassengerId\" : test['PassengerId'], \"Survived\" : y_predicted}).astype('int32')\n",
    "# submission.info()\n",
    "submission.to_csv(\"submission_RF.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8841f7bafa9d104dbee62c6cf5f62fae5061e5f4a5ff02b7a6a5ce4bc4f60264"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
